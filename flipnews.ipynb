{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNBFQ/iOSfDCNHOuoMdBYer",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuragpandey2005/NLP--FlipNews/blob/main/flipnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjpCwx9Xkr2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"flipitnews-data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jH0AWQhmc3GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()"
      ],
      "metadata": {
        "id": "m4xRkVZqRPuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Article\"]=df[\"Article\"].str.lower()"
      ],
      "metadata": {
        "id": "Haa8DRgZaB4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Category\"].value_counts()"
      ],
      "metadata": {
        "id": "zrdFbls9Rc2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Article\"]"
      ],
      "metadata": {
        "id": "ZfAX9Hm5a-k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_html_tag(text):\n",
        "    pattern= re.compile('<,*,?.>')\n",
        "    return pattern.sub(r\"\",text)\n",
        "df[\"Article\"]=df[\"Article\"].apply(remove_html_tag)\n",
        "df[\"Article\"]"
      ],
      "metadata": {
        "id": "b3tr-HkthgAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "WBvIQ6BKm8D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Article\"]"
      ],
      "metadata": {
        "id": "xSxzEPWgpTsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise the count of number of plots\n",
        "def plot_count_category(category_count):\n",
        "    category_count=df[\"Category\"].value_counts()\n",
        "    plt.figure(figsize=(10,5))\n",
        "    category_count.plot(kind='bar',color=\"skyblue\",edgecolor=\"black\")\n",
        "    plt.title(\"Number of News Article per Category\")\n",
        "    plt.xlabel(\"Category\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(axis=\"y\",linestyle=\"--\",alpha=0.7)\n",
        "    plt.show()\n",
        "category_count=df[\"Category\"].value_counts()\n",
        "plot_count_category(category_count)"
      ],
      "metadata": {
        "id": "6vMbZ1OKOegu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cde14fe"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "JkNh5nfapLMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "df[\"Article_cleaned\"] = df[\"Article\"].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "MSOdSvHCpyUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Article\"]"
      ],
      "metadata": {
        "id": "YkTqrrDIp77j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "g8fRlcr2rz_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "zh1j6eJwteVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tokens\"]=df[\"Article\"].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "U9Lpgg0o_RGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"Article\",\"tokens\"]]"
      ],
      "metadata": {
        "id": "X56TCy-I_g7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import  WordNetLemmatizer\n",
        "lematizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ba4KZ3P8ALpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33482fa9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizer_words(word_list):\n",
        "    return [lematizer.lemmatize(word)for word in word_list]\n",
        "df[\"lemmatized_tokens\"]=df[\"tokens\"].apply(lematizer_words)"
      ],
      "metadata": {
        "id": "5HEP-gzp29hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "KE76nT8jQKtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n"
      ],
      "metadata": {
        "id": "bSZagakQ1AWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import  CountVectorizer"
      ],
      "metadata": {
        "id": "LkxqZQaPBxQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer(max_features=5000)"
      ],
      "metadata": {
        "id": "yCUJEJKSEomY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_bow=vectorizer.fit_transform(df_bow[\"lemmatized_tokens\"].apply(lambda x: ' '.join(x)))"
      ],
      "metadata": {
        "id": "hkbgwri2Et3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_df=pd.DataFrame(x_bow.toarray(), columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "LYzIsq7cE_3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bow=pd.concat([df,bow_df],axis=1)"
      ],
      "metadata": {
        "id": "wY4bZwoiF0bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bow"
      ],
      "metadata": {
        "id": "EhQYdK7qGBoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9x17wX_U1ZzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bo_df"
      ],
      "metadata": {
        "id": "Z4-a7gX6GbPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"lemmatized_tokens\"].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "eE3p4JnMGhdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df"
      ],
      "metadata": {
        "id": "BUkWvwI9G3fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "model=LabelEncoder()\n",
        "df[\"label\"]=model.fit_transform(df[[\"Category\"]])"
      ],
      "metadata": {
        "id": "oxi7A-S6G96-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=tfidf_df\n",
        "y=df[\"label\"]"
      ],
      "metadata": {
        "id": "ZyRy9IQQ7o1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VPBC9fNUzr1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "ZXnhD5F-53le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=MultinomialNB()"
      ],
      "metadata": {
        "id": "j97DZX1l6xd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "model=MultinomialNB()\n",
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "bEPMJaBv7Hy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "wJ6NF--z67YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "8hl1zhAd9CSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=RandomForestClassifier()"
      ],
      "metadata": {
        "id": "2HseKGjU9VUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "Fbejjd6y9fj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "AAg9hFso9lEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "rDTzt6v79tYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "4mgjy1xp9xsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_KN=KNeighborsClassifier()"
      ],
      "metadata": {
        "id": "ey8-1YM8_W1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_KN.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "r5m0kuvz_b19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_KN=model.predict(x_test)"
      ],
      "metadata": {
        "id": "4QfQqRpoU0BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_KN))\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_KN))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_KN)"
      ],
      "metadata": {
        "id": "WaSrG2JyU9QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "jodhdXwhVAfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_DT=DecisionTreeClassifier(max_depth=15)"
      ],
      "metadata": {
        "id": "9LzfUsYgVQBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_DT.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "y3oJspVxVV0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DT=model_DT.predict(x_test)"
      ],
      "metadata": {
        "id": "VhZFydPvVZ8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,y_pred_DT)"
      ],
      "metadata": {
        "id": "w0MoNk-WVhCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_DT))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_DT)\n",
        "cm"
      ],
      "metadata": {
        "id": "sSVpCtImVzP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)  # 'linear' works well for text\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_svm = svm_model.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"SVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "at3Z0uNtV2xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0t-gQm2Wlhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "Insights-\n",
        "Best Model: Random Forest\n",
        "\n",
        "Achieves the highest accuracy (97.18%).\n",
        "Shows strong generalization and handles complex decision boundaries effectively.\n",
        "Naïve Bayes (96.62%) performs almost as well as Random Forest.\n",
        "\n",
        "Works well when features are assumed to be independent.\n",
        "Slightly lower recall for class 4 suggests minor misclassifications.\n",
        "KNN (95.30%)\n",
        "\n",
        "Surprisingly performs well, but can be computationally expensive with large datasets.\n",
        "Slightly lower precision for some classes.\n",
        "Decision Tree (80.26%)\n",
        "\n",
        "Performs the worst.\n",
        "Lower recall for class 4, suggesting it struggles with capturing complex patterns.\n",
        "\n",
        "\n",
        "Key Takeaways\n",
        "\n",
        "- Random Forest is the best model, offering high accuracy and balance across all metrics.\n",
        "\n",
        "- Naïve Bayes is a close second and a good alternative for text data.\n",
        "\n",
        "- KNN is surprisingly strong but may struggle with high-dimensional data.\n",
        "\n",
        "- Decision Tree underperforms, reinforcing why ensemble methods like RF are preferred."
      ],
      "metadata": {
        "id": "UBlqVT6RTFxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How many news articles are present in the dataset that we have?\n",
        "\n",
        "ans--2225 article are present\n",
        "\n",
        "after remove duplicates-2126"
      ],
      "metadata": {
        "id": "iTpufWfyTJzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of articles belong to the ‘Technology’ category.\n",
        "\n",
        "ans-401 articles\n"
      ],
      "metadata": {
        "id": "hV8awTB1TXLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What  are Stop Words and why should they be removed from the text data?\n",
        "\n",
        "ans-Stop words are common words (e.g., is, the, and, in, of) that do not       add significant meaning to a text. They should be removed because:\n",
        "\n",
        "----They appear frequently and can add noise.\n",
        "----They do not contribute to understanding the core meaning of a      sentence.\n",
        "-----Removing them improves model efficiency and reduces computational cost."
      ],
      "metadata": {
        "id": "nYhnYUiUTZAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain the difference between Stemming and Lemmatization.\n",
        "\n",
        "ans- Stemming-- Reduces words to their root form by cutting off suffixes.\n",
        "\n",
        "         ex-Running → Run, Cats → Cat\n",
        "         \n",
        "         --Uses simple rules (heuristic).\n",
        "         \n",
        "         --Less accurate (can produce non-meaningful words).\n",
        "         \n",
        "         --faster\n",
        "         \n",
        "      Lemmatization--Converts words to their base or dictionary form.\n",
        "      \n",
        "       ex--Running → Run, Better → Good\n",
        "       \n",
        "       --Uses linguistic knowledge (dictionary-based).\n",
        "       \n",
        "       --More accurate (returns real words).\n",
        "       \n",
        "       ---Slower due to complexity.\n",
        "       \n",
        "       "
      ],
      "metadata": {
        "id": "xtI5m6cBTi9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which of the techniques Bag of Words or TF-IDF is considered to be more efficient than the other\n",
        "ans-bag of words"
      ],
      "metadata": {
        "id": "Dyw81ekPTmnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What’s the shape of train & test data sets after performing a 75:25 split.\n",
        "\n",
        "--Train set: -- 1,594 samples\n",
        "\n",
        "Test set: --- 532 samples"
      ],
      "metadata": {
        "id": "7Qt8v2JmTvge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which of the following is found to be the best performing model..\n",
        "a. Random Forest b. Nearest Neighbors c. Naive Bayes\n",
        "\n",
        "in bag of words-- Naïve Bayes--98.7%\n",
        "\n",
        "tf-idf--Random Forest--97.18%"
      ],
      "metadata": {
        "id": "7CQ6tp0jTzmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  According to this particular use case, both precision and recall are equally important. (T/F)\n",
        "\n",
        "True\n",
        "\n",
        "In this use case, both precision and recall are equally important because:\n",
        "\n",
        "Precision ensures that the predicted categories are correct, reducing false positives.\n",
        "\n",
        "Recall ensures that all relevant instances are captured, reducing false negatives.\n",
        "\n",
        "Since both metrics contribute to the overall effectiveness of the classification model, we should aim for a high F1-score, which balances precision and recall."
      ],
      "metadata": {
        "id": "EZbux69WT1-L"
      }
    }
  ]
}